{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Synthetic Data Generation\n",
    "\n",
    "## Overview\n",
    "This notebook generates synthetic training data for greenwashing detection using GPT-4o.\n",
    "\n",
    "We create two classes:\n",
    "- **Specific (Label 1)**: Concrete claims with numbers, dates, and measurable targets\n",
    "- **Vague (Label 0)**: General statements with hedging words and no concrete commitments\n",
    "\n",
    "## Output\n",
    "- `train_synthetic.csv` - 80% of data for training\n",
    "- `eval_synthetic.csv` - 20% of data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "OUTPUT_DIR = \"../inputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation parameters\n",
    "SAMPLES_PER_CLASS = 300\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# Prompts for each class\n",
    "PROMPTS = {\n",
    "    \"Specific\": \"\"\"\n",
    "Generate 20 example sentences that asset managers might write in sustainability reports.\n",
    "These should be SPECIFIC claims containing:\n",
    "- Concrete numbers (percentages, amounts)\n",
    "- Specific dates or deadlines (e.g., 'by 2030')\n",
    "- Measurable units (tons CO2, GWh, EUR million)\n",
    "\n",
    "Examples:\n",
    "- 'Reduce carbon emissions by 50% by 2030 compared to 2020 baseline.'\n",
    "- 'Invested EUR 2.5 billion in renewable energy projects in 2023.'\n",
    "\n",
    "Return only the sentences, one per line, no numbering.\n",
    "\"\"\",\n",
    "    \"Vague\": \"\"\"\n",
    "Generate 20 example sentences that asset managers might write in sustainability reports.\n",
    "These should be VAGUE claims containing:\n",
    "- Hedging words (may, might, aim, intend, seek)\n",
    "- General aspirations without concrete targets\n",
    "- No specific numbers or dates\n",
    "\n",
    "Examples:\n",
    "- 'We aim to enhance our sustainability practices over time.'\n",
    "- 'Our goal is to become a leader in responsible investing.'\n",
    "\n",
    "Return only the sentences, one per line, no numbering.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Target: {SAMPLES_PER_CLASS * 2} total samples\")\n",
    "print(f\"Batch size: {BATCH_SIZE} sentences per API call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We use GPT-4o to generate synthetic sentences in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(class_type, batch_size):\n",
    "    \"\"\"\n",
    "    Generate a batch of sentences using GPT-4o.\n",
    "    \n",
    "    Args:\n",
    "        class_type: 'Specific' or 'Vague'\n",
    "        batch_size: Number of sentences to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated sentences\n",
    "    \"\"\"\n",
    "    prompt = PROMPTS[class_type]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in corporate sustainability reporting.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    text = response.choices[0].message.content.strip()\n",
    "    sentences = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "print(\"Generation function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of batches needed\n",
    "batches_per_class = (SAMPLES_PER_CLASS + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "all_data = []\n",
    "\n",
    "print(f\"Starting generation of {SAMPLES_PER_CLASS * 2} samples...\")\n",
    "\n",
    "# Generate Specific Claims (Label 1)\n",
    "for _ in tqdm(range(batches_per_class), desc=\"Generating Specific Claims\"):\n",
    "    sentences = generate_batch(\"Specific\", BATCH_SIZE)\n",
    "    for s in sentences:\n",
    "        all_data.append({\"text\": s, \"label\": 1})\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Generate Vague Claims (Label 0)\n",
    "for _ in tqdm(range(batches_per_class), desc=\"Generating Vague Claims\"):\n",
    "    sentences = generate_batch(\"Vague\", BATCH_SIZE)\n",
    "    for s in sentences:\n",
    "        all_data.append({\"text\": s, \"label\": 0})\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Successfully generated {len(df)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Validation\n",
    "\n",
    "Verify that the generated data meets our quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show random samples from each class\n",
    "print(\"\\nSPECIFIC Claims (Label 1) - Should have numbers/dates/concrete targets:\")\n",
    "specific_samples = df[df['label'] == 1].sample(5)\n",
    "for idx, row in specific_samples.iterrows():\n",
    "    print(f\"  - {row['text']}\")\n",
    "\n",
    "print(\"\\nVAGUE Claims (Label 0) - Should have hedging words:\")\n",
    "vague_samples = df[df['label'] == 0].sample(5)\n",
    "for idx, row in vague_samples.iterrows():\n",
    "    print(f\"  - {row['text']}\")\n",
    "\n",
    "# Check class balance\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "balance_ratio = df['label'].value_counts()[0] / df['label'].value_counts()[1]\n",
    "print(f\"Balance ratio: {balance_ratio:.2f}:1\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['text']).sum()\n",
    "print(f\"\\nDuplicates found: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    print(f\"Dataset size after deduplication: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split data into training (80%) and evaluation (20%) sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with stratification to maintain class balance\n",
    "train_df, eval_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "train_path = os.path.join(OUTPUT_DIR, \"train_synthetic.csv\")\n",
    "eval_path = os.path.join(OUTPUT_DIR, \"eval_synthetic.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(f\"Training data saved to: {train_path}\")\n",
    "print(f\"  - Size: {len(train_df)} samples\")\n",
    "print(f\"  - Class 0: {(train_df['label']==0).sum()}\")\n",
    "print(f\"  - Class 1: {(train_df['label']==1).sum()}\")\n",
    "\n",
    "print(f\"\\nEvaluation data saved to: {eval_path}\")\n",
    "print(f\"  - Size: {len(eval_df)} samples\")\n",
    "print(f\"  - Class 0: {(eval_df['label']==0).sum()}\")\n",
    "print(f\"  - Class 1: {(eval_df['label']==1).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
