{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Scoring Real Annual Reports\n",
    "\n",
    "## Overview\n",
    "This notebook runs the complete Greenwashing Detection pipeline on real-world Annual Reports. \n",
    "It automatically finds all PDF files in the `inputs/` folder, extracts their text, and scores them using a hybrid approach:\n",
    "\n",
    "1.  **Vagueness (VUI):** Uses rule-based NLP to count \"hedging\" words (e.g., \"aim,\" \"might,\" \"intend\").\n",
    "2.  **Specificity (SPI):** Uses our **Fine-Tuned RoBERTa model** to detect concrete, quantitative claims.\n",
    "3.  **Greenwashing Risk (GW):** Combines these two signals into a single risk score (0 to 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "\n",
    "# local 'src' library to the python path so we can import our modules\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Import logic from thesis code\n",
    "from src.parsing import extract_pages, split_sentences\n",
    "from src.sectioning import section_by_headings, collect_section_sentences\n",
    "from src.vui import compute_vui\n",
    "from src.spi import compute_spi_rule\n",
    "from src.gw import aggregate_gw\n",
    "\n",
    "# Load the project configuration (keywords, rules, etc.)\n",
    "with open(\"../config.yml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"Libraries and Configuration was loaded successfully.\")\n",
    "\n",
    "# Ensure Spacy English model is installed for sentence splitting\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading Spacy model (en_core_web_sm)...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model of 2_FineTuning_RoBERTa.ipynb\n",
    "MODEL_PATH = \"../models/gw_finetuned\"\n",
    "\n",
    "# Use GPU (Metal/MPS on Mac) if available, otherwise CPU\n",
    "device = 0 if torch.cuda.is_available() or torch.backends.mps.is_available() else -1\n",
    "print(f\"   Running on device index: {device} (0/-1)\")\n",
    "\n",
    "# Initialize the classification pipeline\n",
    "# We strictly truncate text to 512 tokens to prevent crashes on long sentences\n",
    "classifier = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=MODEL_PATH, \n",
    "    tokenizer=MODEL_PATH, \n",
    "    device=device, \n",
    "    truncation=True, \n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../inputs\"\n",
    "\n",
    "# Get a list of all PDF files in the inputs folder\n",
    "pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith(\".pdf\")]\n",
    "pdf_files.sort()  # Sort alphabetically for consistent order\n",
    "\n",
    "print(f\" Found {len(pdf_files)} reports to analyze:\")\n",
    "for f in pdf_files:\n",
    "    print(f\"   - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroing final scores for every report\n",
    "all_report_scores = []\n",
    "\n",
    "print(f\"Starting analysis of {len(pdf_files)} reports...\")\n",
    "\n",
    "# Loop through every PDF file we have\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing Reports\"):\n",
    "    \n",
    "    # --- Step 1: Extract Metadata (Issuer & Year) from Filename ---\n",
    "    try:\n",
    "        filename_clean = pdf_file.replace(\".pdf\", \"\")\n",
    "        parts = filename_clean.split(\"-\")\n",
    "        year = int(parts[-1])\n",
    "        # The first part is the company name (e.g., dws)\n",
    "        issuer = parts[0].capitalize()\n",
    "    except:\n",
    "        # If the filename is wrong, the whole name as the issuer and a default year will be used\n",
    "        print(f\"Could not parse filename '{pdf_file}'. Using defaults.\")\n",
    "        year = 2023\n",
    "        issuer = filename_clean\n",
    "\n",
    "    # --- Step 2: Parse PDF Text ---\n",
    "    pdf_path = os.path.join(input_dir, pdf_file)\n",
    "    try:\n",
    "        # extract_pages is a function from your src/parsing.py\n",
    "        pages = extract_pages(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 3: Sectioning ---\n",
    "    # Dont analyzing legal footer or table of contents\n",
    "    # section_by_headings from config.yml to find \"Strategy\", \"Risk\", etc.\n",
    "    buckets = section_by_headings(pages, cfg)\n",
    "    target_sections = cfg[\"sectioning\"][\"target_sections\"]\n",
    "    \n",
    "    # Collect all sentences from the relevant sections\n",
    "    all_sentences = []\n",
    "    for sec in target_sections:\n",
    "        sec_pages = buckets.get(sec, [])\n",
    "        if sec_pages:\n",
    "            # Split text into individual sentences\n",
    "            sents = collect_section_sentences(sec_pages, lambda txt: split_sentences(txt, \"en_core_web_sm\"))\n",
    "            all_sentences.extend(sents)\n",
    "            \n",
    "    if not all_sentences:\n",
    "        print(f\"{pdf_file}: No text found in target sections (Strategy, Risk, etc). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 4: Scoring ---\n",
    "    \n",
    "    # Vagueness (VUI) - Rule Based\n",
    "    # Calculates the density of vague words per 1000 words\n",
    "    vui_res = compute_vui(all_sentences, cfg)\n",
    "    vui_score = vui_res[\"vui_norm\"]\n",
    "    \n",
    "    # Specificity (SPI) - Hybrid Approach\n",
    "    # Part 1: Rules (Does it have numbers? Dates?, ... )\n",
    "    spi_res = compute_spi_rule(all_sentences, cfg)\n",
    "    \n",
    "    # Part 2: AI (Asks the model: \"Is this specific?\")\n",
    "    # Grabing just the text from the sentence objects\n",
    "    texts = [s[\"text\"] for s in all_sentences]\n",
    "    \n",
    "    # Run the classifier in batches (faster than one by one)\n",
    "    preds = classifier(texts, batch_size=16)\n",
    "    \n",
    "    # Count how many sentences were labeled \"Specific\" (LABEL_1)\n",
    "    count_specific = sum(1 for p in preds if p[\"label\"] == \"LABEL_1\")\n",
    "    ai_spi_score = count_specific / len(all_sentences)\n",
    "    \n",
    "    # Combine Rule-based and AI-based specificity\n",
    "    # (Weights are from current bachelor thesis config: 60% Rules, 40% AI)\n",
    "    spi_hybrid = (0.6 * spi_res[\"spi_rule\"]) + (0.4 * ai_spi_score)\n",
    "    \n",
    "    # C. Final Greenwashing Risk Score\n",
    "    # Formula: Risk increases with Vagueness and decreases with Specificity\n",
    "    gw_score = (0.56 * vui_score) + (0.44 * (1 - spi_hybrid))\n",
    "    \n",
    "    # Save the data\n",
    "    all_report_scores.append({\n",
    "        \"Issuer\": issuer,\n",
    "        \"Year\": year,\n",
    "        \"Filename\": pdf_file,\n",
    "        \"VUI_Score\": round(vui_score, 3),\n",
    "        \"SPI_Score\": round(spi_hybrid, 3),\n",
    "        \"GW_Risk_Score\": round(gw_score, 3),\n",
    "        \"Sentence_Count\": len(all_sentences)\n",
    "    })\n",
    "\n",
    "print(f\"\\n Processing complete. Analyzed {len(all_report_scores)} reports successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pandas DataFrame to view the results\n",
    "df_scores = pd.DataFrame(all_report_scores)\n",
    "\n",
    "# Saving to CSV so we can use it in Notebook 4 (SFDR correlation)\n",
    "output_path = \"../outputs/gw_scores_all.csv\"\n",
    "df_scores.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Show the table sorted by Risk Score (Highest Risk first)\n",
    "display(df_scores.sort_values(\"GW_Risk_Score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1 - Scores by Issuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar chart - average GW Risk per Issuer\n",
    "# Group by Issuer just in case we have multiple years per issuer\n",
    "avg_scores = df_scores.groupby(\"Issuer\")[\"GW_Risk_Score\"].mean().sort_values()\n",
    "\n",
    "colors = ['green' if x < 0.4 else 'orange' if x < 0.6 else 'red' for x in avg_scores]\n",
    "avg_scores.plot(kind='barh', color=colors)\n",
    "\n",
    "plt.title(\"Average Greenwashing Risk by Asset Manager (2021-2024)\")\n",
    "plt.xlabel(\"Greenwashing Risk Score\")\n",
    "plt.ylabel(\"Issuer\")\n",
    "plt.axvline(x=0.5, color='grey', linestyle='--', alpha=0.5, label=\"Risk Threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2 - Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot for multiple years of data\n",
    "if df_scores['Year'].nunique() > 1:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Line chart tracking scores over time for each issuer\n",
    "    sns.lineplot(data=df_scores, x=\"Year\", y=\"GW_Risk_Score\", hue=\"Issuer\", marker=\"o\", linewidth=2.5)\n",
    "    \n",
    "    plt.title(\"Greenwashing Trends Over Time\")\n",
    "    plt.ylabel(\"Greenwashing Risk Score\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    \n",
    "    # Force integer ticks for years (2021, 2022...) instead of decimals (2021.5)\n",
    "    plt.xticks(df_scores['Year'].unique())\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough multi-year data to plot trends yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
