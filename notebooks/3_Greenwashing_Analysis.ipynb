{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Greenwashing Analysis of Real Reports\n",
    "\n",
    "## Overview\n",
    "This notebook applies our fine-tuned greenwashing detection model to real-world annual reports from asset managers.\n",
    "\n",
    "## Pipeline\n",
    "1. Load fine-tuned RoBERTa model\n",
    "2. Process PDF reports (extract text, section into relevant chapters)\n",
    "3. Calculate hybrid scores:\n",
    "   - VUI (Vagueness Index) - rule-based hedging word detection\n",
    "   - SPI (Specificity Index) - AI + rule-based concrete claim detection\n",
    "   - GW (Greenwashing Risk) - combined metric\n",
    "4. Visualize and analyze results\n",
    "\n",
    "## Companies Analyzed\n",
    "BlackRock, Amundi, DWS, Schroders, State Street, KKR, Blackstone, CITIC (2021-2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Import custom modules from thesis code\n",
    "from src.parsing import extract_pages, split_sentences\n",
    "from src.sectioning import section_by_headings, collect_section_sentences\n",
    "from src.vui import compute_vui\n",
    "from src.spi import compute_spi_rule\n",
    "\n",
    "# Load configuration\n",
    "with open(\"../config.yml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"../models/gw_finetuned\"\n",
    "\n",
    "# Load tokenizer and model separately for better control\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def classify_texts(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Classify texts with proper truncation handling.\n",
    "    Returns list of predictions with label and score.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            pred_labels = predictions.argmax(dim=-1).cpu().numpy()\n",
    "            pred_scores = predictions.max(dim=-1).values.cpu().numpy()\n",
    "        \n",
    "        # Format results\n",
    "        for label, score in zip(pred_labels, pred_scores):\n",
    "            results.append({\n",
    "                \"label\": f\"LABEL_{label}\",\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f\"Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "Test the model on sample sentences to verify it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL VALIDATION ON SAMPLE SENTENCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"We reduced emissions by 42% compared to 2020 baseline.\",\n",
    "    \"We are committed to environmental sustainability.\",\n",
    "    \"Our fund targets net-zero by 2050.\",\n",
    "    \"Installed 500 solar panels generating 2.3 GWh annually.\",\n",
    "    \"We aim to enhance our ESG practices over time.\"\n",
    "]\n",
    "\n",
    "predictions = classify_texts(test_sentences)\n",
    "\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    label = \"SPECIFIC\" if pred['label'] == 'LABEL_1' else \"VAGUE\"\n",
    "    confidence = pred['score']\n",
    "    print(f\"\\n[{label}] (confidence: {confidence:.2%})\")\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "print(\"\\nValidation complete. Model is working as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find PDF Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../inputs\"\n",
    "\n",
    "# Get all PDF files\n",
    "pdf_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "print(f\"Found {len(pdf_files)} reports to analyze:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"  - {pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Loop\n",
    "\n",
    "Process each PDF report through the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report_scores = []\n",
    "\n",
    "print(f\"Starting analysis of {len(pdf_files)} reports...\")\n",
    "\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Processing Reports\"):\n",
    "    \n",
    "    # Extract metadata from filename\n",
    "    try:\n",
    "        filename_clean = pdf_file.replace(\".pdf\", \"\")\n",
    "        parts = filename_clean.split(\"-\")\n",
    "        year = int(parts[-1])\n",
    "        issuer = parts[0].capitalize()\n",
    "    except:\n",
    "        print(f\"Could not parse filename '{pdf_file}'. Using defaults.\")\n",
    "        year = 2023\n",
    "        issuer = filename_clean\n",
    "\n",
    "    # Parse PDF text\n",
    "    pdf_path = os.path.join(input_dir, pdf_file)\n",
    "    try:\n",
    "        pages = extract_pages(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Section the document\n",
    "    buckets = section_by_headings(pages, cfg)\n",
    "    target_sections = cfg[\"sectioning\"][\"target_sections\"]\n",
    "    \n",
    "    # Collect sentences from relevant sections\n",
    "    all_sentences = []\n",
    "    for sec in target_sections:\n",
    "        sec_pages = buckets.get(sec, [])\n",
    "        if sec_pages:\n",
    "            sents = collect_section_sentences(\n",
    "                sec_pages,\n",
    "                lambda txt: split_sentences(txt, \"en_core_web_sm\")\n",
    "            )\n",
    "            all_sentences.extend(sents)\n",
    "            \n",
    "    if not all_sentences:\n",
    "        print(f\"{pdf_file}: No text found in target sections. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate VUI (Vagueness Index) using rule-based approach\n",
    "    vui_res = compute_vui(all_sentences, cfg)\n",
    "    vui_score = vui_res[\"vui_norm\"]\n",
    "    \n",
    "    # Calculate SPI (Specificity Index) - Hybrid approach\n",
    "    # Part 1: Rule-based (numbers, dates, units)\n",
    "    spi_res = compute_spi_rule(all_sentences, cfg)\n",
    "    \n",
    "    # Part 2: AI-based (fine-tuned model)\n",
    "    texts = [s[\"text\"] for s in all_sentences]\n",
    "    preds = classify_texts(texts)\n",
    "    count_specific = sum(1 for p in preds if p[\"label\"] == \"LABEL_1\")\n",
    "    ai_spi_score = count_specific / len(all_sentences)\n",
    "    \n",
    "    # Combine rule-based and AI-based SPI (60/40 split)\n",
    "    spi_hybrid = (0.6 * spi_res[\"spi_rule\"]) + (0.4 * ai_spi_score)\n",
    "    \n",
    "    # Calculate final Greenwashing Risk Score\n",
    "    # Formula: High vagueness + Low specificity = High risk\n",
    "    gw_score = (0.56 * vui_score) + (0.44 * (1 - spi_hybrid))\n",
    "    \n",
    "    # Store results\n",
    "    all_report_scores.append({\n",
    "        \"Issuer\": issuer,\n",
    "        \"Year\": year,\n",
    "        \"Filename\": pdf_file,\n",
    "        \"VUI_Score\": round(vui_score, 3),\n",
    "        \"SPI_Score\": round(spi_hybrid, 3),\n",
    "        \"GW_Risk_Score\": round(gw_score, 3),\n",
    "        \"Sentence_Count\": len(all_sentences)\n",
    "    })\n",
    "\n",
    "print(f\"\\nProcessing complete. Analyzed {len(all_report_scores)} reports successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(all_report_scores)\n",
    "results_df = results_df.sort_values(['Issuer', 'Year'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GREENWASHING RISK SCORES BY COMPANY\")\n",
    "print(\"=\"*60)\n",
    "display(results_df)\n",
    "\n",
    "# Save results\n",
    "output_path = \"../outputs/greenwashing_scores.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Descriptive statistics\n",
    "summary = results_df[['VUI_Score', 'SPI_Score', 'GW_Risk_Score']].describe()\n",
    "print(\"\\n\", summary)\n",
    "\n",
    "# Top and bottom performers\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"LOWEST GREENWASHING RISK (Best Performers):\")\n",
    "print(\"-\"*60)\n",
    "best = results_df.nsmallest(5, 'GW_Risk_Score')[['Issuer', 'Year', 'GW_Risk_Score']]\n",
    "print(best.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"HIGHEST GREENWASHING RISK (Worst Performers):\")\n",
    "print(\"-\"*60)\n",
    "worst = results_df.nlargest(5, 'GW_Risk_Score')[['Issuer', 'Year', 'GW_Risk_Score']]\n",
    "print(worst.to_string(index=False))\n",
    "\n",
    "# Average by company\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"AVERAGE GREENWASHING RISK BY COMPANY (2021-2024):\")\n",
    "print(\"-\"*60)\n",
    "avg_by_company = results_df.groupby('Issuer')['GW_Risk_Score'].mean().sort_values()\n",
    "for company, score in avg_by_company.items():\n",
    "    print(f\"{company:20s}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Average GW Risk by Company\n",
    "ax1 = axes[0, 0]\n",
    "avg_by_issuer = results_df.groupby('Issuer')['GW_Risk_Score'].mean().sort_values()\n",
    "avg_by_issuer.plot(kind='barh', ax=ax1, color='coral', edgecolor='black')\n",
    "ax1.set_xlabel('Average GW Risk Score', fontsize=11)\n",
    "ax1.set_ylabel('Company', fontsize=11)\n",
    "ax1.set_title('Greenwashing Risk by Company (2021-2024 Average)', fontsize=13, fontweight='bold')\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, linewidth=2, label='High Risk Threshold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Trend over Time\n",
    "ax2 = axes[0, 1]\n",
    "for issuer in results_df['Issuer'].unique():\n",
    "    issuer_data = results_df[results_df['Issuer'] == issuer].sort_values('Year')\n",
    "    ax2.plot(issuer_data['Year'], issuer_data['GW_Risk_Score'], \n",
    "             marker='o', linewidth=2, markersize=6, label=issuer)\n",
    "ax2.set_xlabel('Year', fontsize=11)\n",
    "ax2.set_ylabel('GW Risk Score', fontsize=11)\n",
    "ax2.set_title('Greenwashing Risk Trends (2021-2024)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. VUI vs SPI Scatter Plot\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(\n",
    "    results_df['SPI_Score'], \n",
    "    results_df['VUI_Score'], \n",
    "    c=results_df['GW_Risk_Score'], \n",
    "    cmap='RdYlGn_r', \n",
    "    s=150, \n",
    "    alpha=0.7, \n",
    "    edgecolors='black',\n",
    "    linewidth=1\n",
    ")\n",
    "ax3.set_xlabel('Specificity Score (SPI)', fontsize=11)\n",
    "ax3.set_ylabel('Vagueness Score (VUI)', fontsize=11)\n",
    "ax3.set_title('Vagueness vs Specificity\\n(Color = GW Risk)', fontsize=13, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax3)\n",
    "cbar.set_label('GW Risk Score', fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribution of GW Risk Scores\n",
    "ax4 = axes[1, 1]\n",
    "results_df['GW_Risk_Score'].hist(\n",
    "    bins=15, \n",
    "    ax=ax4, \n",
    "    color='skyblue', \n",
    "    edgecolor='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "median_score = results_df['GW_Risk_Score'].median()\n",
    "ax4.axvline(\n",
    "    x=median_score, \n",
    "    color='red', \n",
    "    linestyle='--', \n",
    "    linewidth=2,\n",
    "    label=f\"Median: {median_score:.3f}\"\n",
    ")\n",
    "ax4.set_xlabel('GW Risk Score', fontsize=11)\n",
    "ax4.set_ylabel('Frequency', fontsize=11)\n",
    "ax4.set_title('Distribution of Greenwashing Risk Scores', fontsize=13, fontweight='bold')\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "viz_path = '../outputs/greenwashing_analysis.png'\n",
    "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to: {viz_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Interpret the results and draw conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall statistics\n",
    "mean_gw = results_df['GW_Risk_Score'].mean()\n",
    "median_gw = results_df['GW_Risk_Score'].median()\n",
    "std_gw = results_df['GW_Risk_Score'].std()\n",
    "\n",
    "print(f\"\\n1. OVERALL METRICS:\")\n",
    "print(f\"   Mean GW Risk: {mean_gw:.3f}\")\n",
    "print(f\"   Median GW Risk: {median_gw:.3f}\")\n",
    "print(f\"   Std Dev: {std_gw:.3f}\")\n",
    "\n",
    "# High risk companies\n",
    "high_risk = results_df[results_df['GW_Risk_Score'] > 0.6]\n",
    "high_risk_pct = len(high_risk) / len(results_df) * 100\n",
    "\n",
    "print(f\"\\n2. HIGH RISK CLASSIFICATION:\")\n",
    "print(f\"   Reports with GW Risk > 0.6: {len(high_risk)} ({high_risk_pct:.1f}%)\")\n",
    "if len(high_risk) > 0:\n",
    "    print(f\"   Companies: {', '.join(high_risk['Issuer'].unique())}\")\n",
    "\n",
    "# Temporal trend\n",
    "trend = results_df.groupby('Year')['GW_Risk_Score'].mean()\n",
    "trend_direction = \"DECREASING\" if trend.iloc[-1] < trend.iloc[0] else \"INCREASING\"\n",
    "\n",
    "print(f\"\\n3. TEMPORAL TREND:\")\n",
    "print(f\"   Average GW Risk by year:\")\n",
    "for year, score in trend.items():\n",
    "    print(f\"     {year}: {score:.3f}\")\n",
    "print(f\"   Overall trend: {trend_direction}\")\n",
    "\n",
    "# Component analysis\n",
    "mean_vui = results_df['VUI_Score'].mean()\n",
    "mean_spi = results_df['SPI_Score'].mean()\n",
    "\n",
    "print(f\"\\n4. COMPONENT ANALYSIS:\")\n",
    "print(f\"   Average Vagueness (VUI): {mean_vui:.3f}\")\n",
    "print(f\"   Average Specificity (SPI): {mean_spi:.3f}\")\n",
    "\n",
    "dominant_factor = \"Vagueness\" if mean_vui > (1 - mean_spi) else \"Lack of Specificity\"\n",
    "print(f\"   Dominant risk factor: {dominant_factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates the application of a fine-tuned LLM for detecting greenwashing in corporate sustainability reports. The hybrid approach (combining rule-based NLP with AI classification) provides a quantitative measure of disclosure quality across multiple asset managers and years.\n",
    "\n",
    "### Model Performance\n",
    "- Successfully classified specific vs vague claims\n",
    "- Hybrid scoring system captures both linguistic vagueness and lack of concrete commitments\n",
    "\n",
    "### Practical Insights\n",
    "- Significant variation in greenwashing risk across companies\n",
    "- Temporal trends reveal industry-wide shifts in reporting practices\n",
    "- Results can inform investor due diligence and regulatory oversight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
