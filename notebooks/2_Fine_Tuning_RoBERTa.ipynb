{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Fine-Tuning RoBERTa\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes a distilroberta-base model for greenwashing detection.\n",
    "\n",
    "## Process\n",
    "1. Load synthetic training and evaluation data\n",
    "2. Evaluate BASELINE (untrained model) performance\n",
    "3. Fine-tune the model\n",
    "4. Evaluate FINE-TUNED model performance\n",
    "5. Compare baseline vs fine-tuned results\n",
    "6. Analyze errors\n",
    "\n",
    "## Requirements Met\n",
    "- Apply model to dataset\n",
    "- Calculate metrics (Accuracy, Precision, Recall, F1)\n",
    "- Fine-tune model\n",
    "- Demonstrate improvement over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data paths\n",
    "MODEL_NAME = \"distilroberta-base\"\n",
    "TRAIN_PATH = \"../inputs/train_synthetic.csv\"\n",
    "EVAL_PATH = \"../inputs/eval_synthetic.csv\"\n",
    "OUTPUT_DIR = \"../models/gw_finetuned\"\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "eval_df = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Evaluation set: {len(eval_df)} samples\")\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[['label', 'label']])\n",
    "\n",
    "print(\"\\nDatasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text with padding and truncation.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization to both datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a dictionary for easier access\n",
    "tokenized_datasets = {\n",
    "    \"train\": tokenized_train,\n",
    "    \"test\": tokenized_eval\n",
    "}\n",
    "\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate accuracy from predictions.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=10,\n",
    "    use_cpu=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation (Untrained Model)\n",
    "\n",
    "Evaluate the model BEFORE training to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BASELINE EVALUATION - UNTRAINED MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate untrained model\n",
    "baseline_results = trainer.evaluate()\n",
    "baseline_preds = trainer.predict(tokenized_datasets[\"test\"])\n",
    "baseline_pred_labels = baseline_preds.predictions.argmax(-1)\n",
    "baseline_true_labels = baseline_preds.label_ids\n",
    "\n",
    "# Store baseline accuracy for comparison\n",
    "baseline_acc = baseline_results['eval_accuracy']\n",
    "\n",
    "print(f\"\\nBaseline Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"Baseline Loss: {baseline_results['eval_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Baseline):\")\n",
    "print(classification_report(\n",
    "    baseline_true_labels,\n",
    "    baseline_pred_labels,\n",
    "    target_names=[\"Vague\", \"Specific\"],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Baseline):\")\n",
    "print(confusion_matrix(baseline_true_labels, baseline_pred_labels))\n",
    "print(\"[[TN FP]\")\n",
    "print(\" [FN TP]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Train the model on our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nFine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuned Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "final_results = trainer.evaluate()\n",
    "final_preds = trainer.predict(tokenized_datasets[\"test\"])\n",
    "final_pred_labels = final_preds.predictions.argmax(-1)\n",
    "final_true_labels = final_preds.label_ids\n",
    "\n",
    "print(f\"\\nFine-tuned Accuracy: {final_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned Loss: {final_results['eval_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Fine-tuned):\")\n",
    "print(classification_report(\n",
    "    final_true_labels,\n",
    "    final_pred_labels,\n",
    "    target_names=[\"Vague\", \"Specific\"],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Fine-tuned):\")\n",
    "print(confusion_matrix(final_true_labels, final_pred_labels))\n",
    "print(\"[[TN FP]\")\n",
    "print(\" [FN TP]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Analysis\n",
    "\n",
    "Compare baseline vs fine-tuned performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENT ANALYSIS: Baseline vs Fine-tuned\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improvement = final_results['eval_accuracy'] - baseline_acc\n",
    "improvement_pct = improvement * 100\n",
    "\n",
    "print(f\"\\nBaseline Accuracy:     {baseline_acc:.4f}\")\n",
    "print(f\"Fine-tuned Accuracy:   {final_results['eval_accuracy']:.4f}\")\n",
    "print(f\"\\nAbsolute Improvement:  +{improvement:.4f}\")\n",
    "print(f\"Relative Improvement:  +{improvement_pct:.2f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"\\nRESULT: Fine-tuning successfully improved model performance.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No improvement observed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Examine misclassified examples to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find misclassifications\n",
    "errors = []\n",
    "for i, (pred, true) in enumerate(zip(final_pred_labels, final_true_labels)):\n",
    "    if pred != true:\n",
    "        errors.append({\n",
    "            \"text\": eval_df.iloc[i][\"text\"],\n",
    "            \"predicted\": \"Specific\" if pred == 1 else \"Vague\",\n",
    "            \"actual\": \"Specific\" if true == 1 else \"Vague\"\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal Misclassifications: {len(errors)} / {len(final_true_labels)}\")\n",
    "print(f\"Error Rate: {len(errors)/len(final_true_labels)*100:.2f}%\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nExample Misclassifications:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, err in enumerate(errors[:5], 1):\n",
    "        print(f\"\\nError {i}:\")\n",
    "        print(f\"Text: {err['text'][:150]}...\" if len(err['text']) > 150 else f\"Text: {err['text']}\")\n",
    "        print(f\"Predicted: {err['predicted']} | Actual: {err['actual']}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"\\nNo errors found - perfect classification on evaluation set.\")\n",
    "    print(\"Note: This may indicate overfitting on synthetic data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nFine-tuned model saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nModel is ready for use in Notebook 3: Greenwashing Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Baseline evaluation of untrained model\n",
    "2. Fine-tuning on synthetic greenwashing data\n",
    "3. Comprehensive metrics (Accuracy, Precision, Recall, F1)\n",
    "4. Quantified improvement over baseline\n",
    "5. Error analysis of misclassifications\n",
    "\n",
    "The model is now ready to classify real corporate sustainability reports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
